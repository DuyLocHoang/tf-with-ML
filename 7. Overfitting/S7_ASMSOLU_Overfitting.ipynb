{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S7_ASMSOLU_Overfitting.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8rMQGrU_Ct0"
      },
      "source": [
        "<h1><center>\n",
        "\n",
        "![](https://i.imgur.com/jrRGMfx.png)\n",
        "\n",
        "AI PRACTITIONER COURSE \n",
        "\n",
        "ASSIGNMENT - SESSION 7 - More on Search: Overfitting Fighting Techniques\n",
        "\n",
        "**&copy; 2020 VTCA-COTAI. Internal Use Only.**\n",
        "\n",
        "</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8ZnanvfV8ml"
      },
      "source": [
        "# 7.1 [5 Points] Theory Review\n",
        "\n",
        "## 7.1.1  [1 Point] Generalization\n",
        "\n",
        "- What does generalization mean? Khái quát hóa trong ML nghĩa là gì?\n",
        "- What does underfitting, overfitting mean? Chưa khớp & quá khớp trong ML nghĩa là gì?\n",
        "- How to prevent underfitting? Làm sao để tránh chưa khớp? \n",
        "\n",
        "**Answers**: \n",
        "- Generalization in ML means performing predictions equally well on unseen inputs (test set) as on training data. \n",
        "- Khái quát hóa là khả năng dự báo của mô hình khi triển khai thực tế trên dữ liệu mới tốt tương đương với khi học/huấn luyện. \n",
        "\n",
        "- Underfitting is when ML models are too simple to capture the underlying complexity of the function generating data, resulting in poor predictive performance.  \n",
        "- Chưa khớp là khi mô hình ML quá đơn giản không thể xấp xỉ tốt hàm ẩn tối ưu, nên độ chính xác trong dự báo là không cao. \n",
        "\n",
        "- Underfitting can be prevented by increasing model complexity, e.g., by using compositional, nonlinear functions, or by using more features. \n",
        "- Chưa khớp có thể tránh bằng cách tăng độ phức tạp của mô hình lên 1 mức phù hợp, ví dụ sử dụng hàm phi tuyến nhiều lớp, dùng nhiều đặc trưng hơn.\n",
        "\n",
        "- Overfitting is when ML models are overly complicated they tend to memorize the training data, resulting in low generalization capability.\n",
        "- Quá khớp là khi mô hình ML phức tạp quá mức cần thiết, trở nên ghi nhớ dữ liệu huấn luyện, do đó khả năng khái quát hóa không cao.\n",
        "\n",
        "## 7.1.2  [4 Points] Techniques to prevent overfitting \n",
        "Briefly describe each technique and why it helps fix overfitting. Mô tả ngắn gọn từng phương pháp chống quá khớp, và vì sao nó giúp tránh quá khớp. \n",
        "- [0.5 point] feature selection - chọn lại đặc trưng.\n",
        "- [1 point] regularization - kiểm soát độ phức tạp của mô hình.\n",
        "- [0.5 point] early stopping - dừng sớm.\n",
        "- [1 point] dropout - tắt ngẫu nhiên.\n",
        "- [1 point] batchnorm - chuẩn hóa theo lô.\n",
        "\n",
        "**Answers**: \n",
        "- Feature selection removes unimportant features, hence reducing model complexity.\n",
        "- Phương pháp \"chọn lại đặc trưng\" giúp loại bỏ những đặc trưng không quan trọng, nên mô hình bớt phức tạp hơn $\\to$ giảm quá khớp.\n",
        "\n",
        "- Regularization adds a penality term for model complexity to the loss function to optimize, forcing the search algorithm to balance between training accuracy and simplicity of the solution model.\n",
        "- Phương pháp \"kiểm soát độ phức tạp\" thêm một độ đo về độ phức tạp của mô hình vào hàm mục tiêu để tối ưu, do đó giữa những hàm có  cùng độ chính xác trên dữ liệu huấn luyện thì hàm được chọn sẽ đơn giản hơn, giúp giảm quá khớp.\n",
        "\n",
        "- Early stopping uses validation set to track generalization capability of ML models and stops training further when the solution model starts overfitting. \n",
        "- Phương pháp \"dừng sớm\" sử dụng bộ dữ liệu thẩm định để theo dõi khả năng khái quát hóa của mô hình và dừng huấn luyện lại ngay khi mô hình có dấu hiệu quá khớp.\n",
        "\n",
        "- Dropout randomly switches off weights/parameters during training (but uses *all* after training, with [activation scaling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)) which effectively adds a regularization term in the training loss.\n",
        "- Phương pháp \"tắt ngẫu nhiên\" chỉ huấn luyện các phần nhỏ ngẫu nhiên của mô hình (và sử dụng toàn bộ mô hình sau khi huấn luyện), tạo ra hiệu ứng tương đương với việc cộng thêm 1 hàm kiểm soát độ phức tạp, giúp chống quá khớp.  \n",
        "\n",
        "- Batchnorm *standardizes* the input batch to each layer of a network, making the optimization landscape significantly smoother and allowing for faster training. See [NeurIPS'18](https://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf).\n",
        "- Phương pháp \"chuẩn hóa theo lô\" giúp ổn định phân bố của đầu vào của mỗi lớp mạng, làm cho bề mặt hàm mục tiêu trơn hơn, dễ tối ưu hơn.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7spO6VoF_MZI"
      },
      "source": [
        "# 7.2 [3 Points] Coding Practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viGDNnJf_pBI"
      },
      "source": [
        "Ở Assignment hôm nay, chúng ta sẽ thực hành dùng Keras để thiết kế mô hình Deep Neural Network cho bài toán phân loại quần áo\n",
        "\n",
        "Dataset được sử dụng là Mnist-Fashion (Có sẵn trên Keras)\n",
        "\n",
        "Mục tiêu của Assignment:\n",
        "\n",
        "\n",
        "*   Luyện tập coding với thư viện Keras\n",
        "*   Áp dụng các kĩ thuật hạn chế Overfit vào mô hình\n",
        "\n",
        "**Lưu ý**\n",
        "\n",
        "Assignment này được thiết kế để học viên có thể thấy được tác dụng của việc áp dụng các kĩ thuật chống overfit vào model. Tuy nhiên do dataset và kiến trúc mô hình quá đơn giản, nên độ hiệu quả của các kỹ thuật áp dụng sẽ rất nhỏ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ctErV4PRpiy"
      },
      "source": [
        "## 7.2.1 [1 Point] Chuẩn bị dữ liệu\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeON3W9uTYmP"
      },
      "source": [
        "from tensorflow.random import set_seed\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I381i_ClSac_"
      },
      "source": [
        "**Label (class) của các tấm hình trong dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETbyJRKwSegz"
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "#['Áo thun', 'Quần dài', 'Áo liền quần', 'Đầm', 'Áo khoác',\n",
        "#               'Sandal', 'Áo sơ mi', 'Giày', 'Túi xách', 'Ủng']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y8dpilGSjor"
      },
      "source": [
        "**Load dataset từ Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKOhKNYe-EFW"
      },
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "(X_train, y_train),(X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So3d9T0XjPhG"
      },
      "source": [
        "**Kiểm tra shape của Train Set và Test Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzEokjudjOt1"
      },
      "source": [
        "# TODO\n",
        "# 1. Print out the shape of X_train, y_train, X_test, y_test\n",
        "print('Shape of X_train:', X_train.shape)\n",
        "print('Shape of y_train:', y_train.shape)\n",
        "print('Shape of X_test:', X_test.shape)\n",
        "print('Shape of y_test:', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9d7YlfBjzya"
      },
      "source": [
        "Từ shape được in ra, ta có thể thấy được dữ liệu là 1 tấm hình đen trắng có kích thước là 28x28 pixels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3TdOa8XVgC9"
      },
      "source": [
        "**Kiểm tra phân bố label giữa Train Set và Test Set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhTTRel1Sp6T"
      },
      "source": [
        "Trong dataset có tổng cộng 10 class, chúng ta cần kiểm tra xem các class có phân bố đồng đều trong dataset hay không?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACn3kW4xARYR"
      },
      "source": [
        "# TODO\n",
        "# 1. Dùng hàm np.unique để kiểm tra\n",
        "# 2. In ra kết quả\n",
        "print('Train Set labels distribution')\n",
        "print(np.unique(y_train, return_counts=True))\n",
        "print('Test Set labels distribution')\n",
        "print(np.unique(y_test, return_counts=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl9EGN9nTt2U"
      },
      "source": [
        "Từ kết quả được in ra, ta thấy được các label được phân bố đều nhau trong cả Train Set và Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYBk6MvyT8EC"
      },
      "source": [
        "**Tạo Validation Set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9cb-FA0UKt0"
      },
      "source": [
        "Ở bước này, ta dùng hàm train_test_split() của sklearn để tách 10000 tấm hình ra khỏi Train Set và dùng nó làm Validation Set\n",
        "\n",
        "Ta có thể dùng test_size=10000 để lấy ra 10000 tấm hình\n",
        "\n",
        "**Note:** hàm train_test_split() cần thông số gì để chia đều các label ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVsDi9ArTVNI"
      },
      "source": [
        "# TODO\n",
        "# 1. import train_test_split from sklearn\n",
        "# 2. split X_train, y_train into new Train Set and Validation Set (1000 items)\n",
        "# 3. use random_state=42\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, stratify=y_train, test_size=10000, shuffle=True, random_state=42\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSnKKF1PVm1h"
      },
      "source": [
        "**Kiểm tra phân bố label 1 lần nữa**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JE3hP2NXEuu"
      },
      "source": [
        "# TODO\n",
        "# 1. Dùng hàm np.unique để kiểm tra phân bố nhãn trên cả 3 set Train, Validation, Test\n",
        "# 2. In ra kết quả\n",
        "print('Train Set labels distribution')\n",
        "print(np.unique(y_train, return_counts=True))\n",
        "print()\n",
        "print('Validation Set labels distribution')\n",
        "print(np.unique(y_val, return_counts=True))\n",
        "print()\n",
        "print('Test Set labels distribution')\n",
        "print(np.unique(y_test, return_counts=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87TsRr2biNGH"
      },
      "source": [
        "**One hot encoding cho label y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSXRTuD-h_Tt"
      },
      "source": [
        "# TODO:\n",
        "# 1. import to_categorical() from keras\n",
        "# 2. Apply encoding for y_train, y_val, y_test\n",
        "# 3. Name the new variables Y_train, Y_val, Y_test\n",
        "# 4. Print out the shape of Y_train, Y_val, Y_test\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "Y_train = to_categorical(y_train, num_classes=10)\n",
        "Y_val = to_categorical(y_val, num_classes=10)\n",
        "Y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "print('Shape of Y_train:', Y_train.shape)\n",
        "print('Shape of Y_val:', Y_val.shape)\n",
        "print('Shape of Y_test:', Y_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPfRKW8GkTqv"
      },
      "source": [
        "**Scale data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eVij_2H4G4p"
      },
      "source": [
        "Đầu tiên ta cần kiểm tra kiểu dữ liệu của X, cũng như giá trị min, max\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-dfooKQkfyO"
      },
      "source": [
        "print('Type of X:', X_train.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk7I1EtQ4WyK"
      },
      "source": [
        "print('Minimum value of X:', np.min(X_train))\n",
        "print('Maximum value of X:', np.max(X_train))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oehiCljZkgIy"
      },
      "source": [
        "Do dữ liệu là hình ảnh, ta chỉ cần dùng MinMaxScaler để scaler về [0,1]\n",
        "\n",
        "$X = \\frac{X - min(X)}{max(X) - min(X)}$\n",
        "\n",
        "Công thức đơn giản\n",
        "\n",
        "$img = \\frac{img}{ 255}$\n",
        "\n",
        "Dữ liệu cũng cần phải chuyển về dạng float"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qR-R-aki4FQ"
      },
      "source": [
        "# TODO\n",
        "# 1. Convert X_train, X_val and X_test to [0, 1] range\n",
        "X_train = X_train / 255.0\n",
        "X_val = X_val / 255.0\n",
        "X_test = X_test / 255.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwHUMdAV9muL"
      },
      "source": [
        "## 7.2.2 Visualize data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1O_7D6DGfyN"
      },
      "source": [
        "Thử in ra 1 vài tấm hình trong dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyAmrkyA9EdO"
      },
      "source": [
        "fig, axs = plt.subplots(10,10)\n",
        "fig.set_figheight(15)\n",
        "fig.set_figwidth(15)\n",
        "for i in range(10):\n",
        "  for j in range(10):\n",
        "    target = np.random.choice(np.where(y_train == i)[0])\n",
        "    axs[i][j].grid('off')\n",
        "    axs[i][j].axis('off')\n",
        "    axs[i][j].imshow(np.squeeze(X_train[target]), cmap='gray')\n",
        "    axs[i][j].set_title(class_names[i])\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbp0PqIkGnoi"
      },
      "source": [
        "## 7.2.3 [2 Points] Thiết kế mô hình DNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmJwfAWLLzN7"
      },
      "source": [
        "**Xây dựng hàm draw_chart() để vẽ Learning Curve cho tập Train và Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsm-iQe3L7kK"
      },
      "source": [
        "# TODO\n",
        "# 1. complete the draw_chart() function below\n",
        "\n",
        "def draw_chart(history):\n",
        "  plt.figure(figsize=(15,8))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Losses vs Epochs')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend(['Train', 'Validation'])\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('Accuracy vs Epochs')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend(['Train', 'Validation'])\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP005xG8MRWg"
      },
      "source": [
        "**Xây dựng kiến trúc và huấn luyện mô hình không có Dropout**\n",
        "\n",
        "Học viên xây dựng mô hình DNN với kiến trúc như sau:\n",
        "- Trong trường hợp dữ liệu là hình ảnh thì layer đầu tiên của mạng DNN là gì ?\n",
        "- Tiếp theo là 3 hidden-layer có 512 units ở mỗi layer\n",
        "- Cuối cùng là output layer\n",
        "\n",
        "Sau đó:\n",
        "- Compile mô hình\n",
        "- Train với 50 epochs (lưu ý dùng validation_data=(X_val, Y_val)\n",
        "- Vẽ learning curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXTqhqjrh0iW"
      },
      "source": [
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.backend import clear_session\n",
        "clear_session()\n",
        "set_seed(42)\n",
        "np.random.seed(42)\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
        "history = model.fit(X_train, Y_train, epochs=50, validation_data=(X_val, Y_val))\n",
        "draw_chart(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2c-01pXl967"
      },
      "source": [
        "Ta thấy được rõ ràng mô hình trên đang bị overfit, tiếp theo ta sẽ áp dụng 3 kỹ thuật là Dropout, Reduce Learning Rate on Plataue và Early Stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3nlekUclYOV"
      },
      "source": [
        "Một vài lưu ý\n",
        "\n",
        "*   Vấn đề đặt layer Dropout ở trước hay sau Activation vẫn đang được tranh luận. Nhưng có 1 best practice được rất nhiều người dùng đó là đặt Dropout phía trước Activation nếu dùng ReLU.\n",
        "\n",
        "```\n",
        "layer Dense\n",
        "layer Dropout\n",
        "layer Activation('relu')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9gC4g5zoBIV"
      },
      "source": [
        "Học viên xây dựng mô hình DNN với kiến trúc như sau:\n",
        "- Trong trường hợp dữ liệu là hình ảnh thì layer đầu tiên của mạng DNN là gì ?\n",
        "- Tiếp theo là 3 hidden-layer có 512 units ở mỗi layer, có layer Dropout chèn giữa Dense và Activation\n",
        "- Cuối cùng là output layer\n",
        "\n",
        "Sau đó:\n",
        "- Compile mô hình\n",
        "- Tạo 2 callback là:\n",
        "  - EarlyStopping, nếu sau 15 epochs, val_loss không giảm thì ngưng, trả ra trọng số ở thời điểm tốt nhất\n",
        "  - ReduceLROnPlateau, nếu sau 3 epochs, val_loss không giảm thì thay đổi learning_rate theo công thức sau ``lr = lr * 0.2``\n",
        "- Do có áp dụng EarlyStopping, ta sẽ train với 1000 epochs (lưu ý dùng validation_data=(X_val, Y_val)\n",
        "- Vẽ learning curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plZvSqVoGayx"
      },
      "source": [
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.backend import clear_session\n",
        "clear_session()\n",
        "set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
        "model.add(Dense(512, kernel_initializer='he_uniform'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(512, kernel_initializer='he_uniform'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(512, kernel_initializer='he_uniform'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.2)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "history = model.fit(X_train, Y_train, epochs=1000, validation_data=(X_val, Y_val), callbacks=[reduce_lr, early_stop])\n",
        "draw_chart(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHhSPHCbr4kc"
      },
      "source": [
        "Quan sát kết quả cuối cùng của 2 model, ta thấy được:\n",
        "\n",
        "**Đối với model có Dropout**\n",
        "*      Trên tập Train, Loss giảm chậm hơn, Accuracy tăng chậm hơn (giảm overfit)\n",
        "\n",
        "\n",
        "*      Accuracy trên tập Validation có thể đạt được kết quả tốt hơn (1 chút) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUmAbATZzmfJ"
      },
      "source": [
        "Quan sát Learning Curve của cả 2 model (Có và không có Dropout) ta thấy được đường Validation của Model có Dropout ổn định hơn so với đường của Model không có Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW0OO09Dk9aE"
      },
      "source": [
        "# TODO \n",
        "# 1. use dropout_model.evaluate() on 3 Set\n",
        "model.evaluate(X_train, Y_train)\n",
        "model.evaluate(X_val, Y_val)\n",
        "model.evaluate(X_test, Y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdT9Hx4kwLQ_"
      },
      "source": [
        "**Lưu ý**\n",
        "\n",
        "Assignment này không chấm điểm dựa vào accuracy cuối cùng của model mà chỉ chẩm điểm coding. Chỉ cần hoàn thành đúng yêu cầu của các phần TODO là được.\n",
        "\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWEzpwU-s7dt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}