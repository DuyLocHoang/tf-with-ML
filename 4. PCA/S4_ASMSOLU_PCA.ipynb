{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S4_ASMSOLU_PCA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfoGLA-RxEfn"
      },
      "source": [
        "<h1><center>\n",
        "\n",
        "![](https://i.imgur.com/jrRGMfx.png)\n",
        "\n",
        "AI PRACTITIONER COURSE \n",
        "\n",
        "ASSIGNMENT - SESSION 4 - PCA\n",
        "\n",
        "**&copy; 2020 VTCA-COTAI. Internal Use Only.**\n",
        "\n",
        "</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB5ywpEc3R9Z"
      },
      "source": [
        "# 4.1. Theory\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4l8YNIi7z2j"
      },
      "source": [
        "## 4.1.1 [1-point] \n",
        "\n",
        "- Explain the methods to compute (dis)similarity between a basis or template and an input.\n",
        "- Tóm tắt các phương pháp so khớp (tính sự giống khác nhau) giữa một mẫu cơ sở và một input.\n",
        "\n",
        "**Answer**:\n",
        "- **Dot product**: linear combination of feature values ($z_i$) and weights ($w_i$).\n",
        "<br>\n",
        "Tích chấm: tổng có trọng số của giá trị các đặc trưng.\n",
        "- **Cosine similarity**: dot product of unit-length scaled inputs.\n",
        "<br>\n",
        "Cosine của góc: tích chấm của 2 inputs đã được chuẩn hóa về độ dài đơn vị.\n",
        "- **Convolution**: dot product + sliding.\n",
        "<br>\n",
        "Tích chập: tích chấm và trượt.\n",
        "- **Distance**: e.g., Euclidean distance = magnitude of the difference vector ($\\|X - B\\|$). \n",
        "<br>\n",
        "Khoảng cách: ví dụ khoảng cách thẳng (Euclid) = độ lớn của hiệu 2 vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1whbHnLf73Q4"
      },
      "source": [
        "## 4.1.2 [1-point] \n",
        "- Explain the 4 main methods to extract features in ML.\n",
        "- Tóm tắt 4 phương pháp chính trích xuất đặc trưng cho ra vector tọa độ nhúng $Z$ \n",
        "\n",
        "**Answer**:\n",
        "- **Sparse coding**: find a dictionary, composed of $n$ templates $B_1,\\dots,B_n$ as \"words\" or \"codebooks\", such that each input $X$ can be well approximated by weighted sum of a very small number of templates. That is, $\\forall X: X\\approx \\sum_{i=1}^n z_i B_i$ such that the number of nonzero coefficients $z_i$, denoted as $k$, is very small: $k\\ll n$.\n",
        "<br>\n",
        "Mã hóa thưa: tìm 1 bộ từ điển gồm $n$ mẫu sao cho mọi input đều có thể được mã hóa (xấp xỉ) bởi vài mẫu, tức là số mẫu cần dùng $k$ rất nhỏ so với số mẫu $n$.\n",
        "- **PCA**: find global templates (called principal components) $B_1,\\dots,B_n$ such that when all inputs $\\{X^t\\}_{t=1}^N$ of the training dataset are approximated by weighted sum of the first $k$ templates, $X^t\\approx \\sum_{i=1}^k z^t_i B_i$, it is the best approximation (with minimum sum of squared errors) for any $k=0,\\dots,n$.\n",
        "<br>\n",
        "Phân tích thành phần chính: tìm $n$ mẫu toàn cục sao cho khi xấp xỉ bộ dữ liệu huấn luyện bằng $k$ thành phần đầu tiên của bộ mẫu thì ta có được cách xấp xỉ có tổng số lỗi nhỏ nhất, với mọi giá trị của $k$ từ 0 đến $n$.\n",
        "- **Convolutional kernels**: extract details as feature maps using small templates to convolve with input.\n",
        "<br>\n",
        "Các bộ lọc tích chập: trích xuất các chi tiết và tạo thành các \"bản đồ đặc trưng\" bằng cách sử dụng các mẫu kích thước nhỏ và chập với input.\n",
        "- **Word2vec**: representing input objects (words, profiles, items, etc.) as embedding vectors $Z = (z_1,\\dots,z_n)$ such that prediction accuracy of downstream tasks using these embeddings are maximized.\n",
        "<br>\n",
        "Vector nhúng: biểu diễn các vật đầu vào (từ ngữ, hồ sơ, sản phẩm v.v.) thành các vector tọa độ nhúng sao cho khi sử dụng chúng để dự báo thì ta đạt được độ chính xác cao.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46L3DoeK76dQ"
      },
      "source": [
        "\n",
        "## 4.1.3 [1-point] \n",
        "- Summarize main points about PCA that you understand.\n",
        "- Tóm tắt các ý chính về PCA mà bạn hiểu.\n",
        "\n",
        "**Answer**:\n",
        "- PCA gives best approximation (minimum sum of squared errors of the training set) when using any $k$ first components.\n",
        "<br>\n",
        "PCA trích xuất bộ mẫu cho xấp xỉ của bộ dữ liệu tốt nhất (tổng sai số bình phương nhỏ nhất) khi sử dụng $k$ thành phần chính đầu tiên, với bất cứ số mẫu $k$ nào.\n",
        "- PCA components (except for the mean) are mutually orthonormal.\n",
        "<br>\n",
        "Các thành phần chính là hệ trực chuẩn.\n",
        "- PCA components are global templates (not local details as in convolutional filters)\n",
        "<br>\n",
        "Các thành phần chính là các mẫu toàn cục, không phải các chi tiết cục bộ.\n",
        "- When projecting the dataset onto each of the principal components (i.e., directions) we have maximum spreadings of the coordinates.\n",
        "<br>\n",
        "Khi chiếu bộ dữ liệu huấn luyện lên từng thành phần chính, ta có các tọa độ nhúng với độ phân tán lớn nhất.\n",
        "\n",
        "![](https://i.imgur.com/Frac4kr.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3QFlr1TxqtY"
      },
      "source": [
        "# 4.2 Coding: Logistic Regression with PCA\n",
        "\n",
        "Nội dung chính:\n",
        "\n",
        "- Chuẩn bị dữ liệu\n",
        "\n",
        "- Áp dụng PCA để giảm chiều dữ liệu\n",
        "\n",
        "- Classification trên PCA features với Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpArrYfiy8mG"
      },
      "source": [
        "## 4.2.1 Chuẩn bị dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4hB47P4x28r"
      },
      "source": [
        "Chạy cell bên dưới để tải dataset về.\n",
        "\n",
        "Ở bài này ta dùng lại dataset Mnist Digits (Phân loại các con số từ 0-9)\n",
        "\n",
        "**Noted: Khi dùng PCA với dữ liệu là Image, ta cần Flatten các tấm hình, bộ dữ liệu bên dưới đã được Flatten sẵn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLE9ugojyT8P"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "from sklearn.datasets import fetch_openml\n",
        "import matplotlib.pyplot as plt\n",
        "mnist = fetch_openml('mnist_784')\n",
        "\n",
        "print('Shape of X:', mnist.data.shape)\n",
        "print('Shape of y:', mnist.target.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sc_mYi0yxOB"
      },
      "source": [
        "X có 784 features tương đương X là tấm hình có size (28x28)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdN1fM3B62Sr"
      },
      "source": [
        "# Constansts\n",
        "img_H = 28\n",
        "img_W = 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljpMwbv1zDA-"
      },
      "source": [
        "**Chia data thành 2 bộ Train và Test** (Dùng test_size = 0.2 và random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JjZR2grw4FI"
      },
      "source": [
        "# TODO:\n",
        "# split data into Train and Test\n",
        "# print out the shape\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size=0.2, random_state=42)\n",
        "print('Shape of X_train', X_train.shape)\n",
        "print('Shape of y_train', y_train.shape)\n",
        "print('Shape of X_test', X_test.shape)\n",
        "print('Shape of y_test', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7s-0XaY8mlw"
      },
      "source": [
        "Convert y_train, y_test từ object sang int"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jc9BtJV8lWa"
      },
      "source": [
        "y_train = y_train.astype(np.uint8)\n",
        "y_test  = y_test.astype(np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qQU9WnV0F0Q"
      },
      "source": [
        "## 4.2.2 [1-points] Áp dụng PCA để giảm chiều dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roY7iJQ5z1e4"
      },
      "source": [
        "**Normalize data**\n",
        "\n",
        "Trước khi dùng PCA, ta luôn phải normalize dataset bằng cách gọi class StandardScaler trong sklearn\n",
        "\n",
        "Link tham khảo: [here](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHYl0X7YC96w"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCyjqDpVzaLe"
      },
      "source": [
        "# TODO\n",
        "# 1. Fit StandardScaler trên Train Set\n",
        "# 2. Transform trên Train Set và Test Set\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "# Fit on training set only.\n",
        "scaler.fit(X_train)\n",
        "# Apply transform to both the training set and the test set.\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b832gVVK0exM"
      },
      "source": [
        "Import PCA từ thư viện sklearn.\n",
        "\n",
        "Khi sử dụng PCA ta có thể:\n",
        "\n",
        "*   Truyền vào số components ta muốn: \n",
        "\n",
        "```\n",
        "pca = PCA(100)\n",
        "```\n",
        "\n",
        "\n",
        "*   Truyền vào % thông tin ta muốn giữ lại:\n",
        "\n",
        "```\n",
        "pca = PCA(0.90)\n",
        "```\n",
        "\n",
        "*   Không truyền tham số:\n",
        "\n",
        "```\n",
        "pca = PCA() #từ đây ta có thể vẽ chart để quyết định sử dụng bao nhiêu components\n",
        "```\n",
        "**Một số câu lệnh phổ biến khi dùng PCA**\n",
        "\n",
        "*   Cơ bản\n",
        "\n",
        "```\n",
        "fit()\n",
        "fit_transform()\n",
        "transform()\n",
        "\n",
        "# Tương tự khi dùng Standard Scaler\n",
        "# Lưu ý khi dùng PCA ta vẫn phải fit() trên train set và transform() trên train set lẫn test set\n",
        "```\n",
        "\n",
        "*   Fit PCA sau đó tái tạo lại dữ liệu ban đầu  \n",
        "\n",
        "```\n",
        "components = pca.fit_transform(data)\n",
        "projected = pca.inverse_transform(components) \n",
        "```\n",
        "\n",
        "*    In ra số component đang dùng \n",
        "\n",
        "```\n",
        "pca.n_components_\n",
        "```\n",
        "*    In ra % lượng thông tin được giữ lại\n",
        "\n",
        "```\n",
        "pca.explained_variance_ratio_\n",
        "```\n",
        "\n",
        "*    Vẽ chart Components versus Lượng thông tin được giữ lại\n",
        "\n",
        "```\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance');\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCeD13ZE0HkU"
      },
      "source": [
        "# TODO:\n",
        "# 1. Khởi tạo PCA với lượng thông tin được giữ lại là 95%\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(0.95)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulKHdeqE3lTb"
      },
      "source": [
        "Áp dụng PCA trên train set sau đó transform trên train set và test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ex71-v743wm"
      },
      "source": [
        "# TODO:\n",
        "# 1. Áp dụng pca cho X_train và X_test\n",
        "\n",
        "pca.fit(X_train)\n",
        "X_train_pca = pca.transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "print('Shape of X_train_pca', X_train_pca.shape)\n",
        "print('Shape of X_test_pca', X_test_pca.shape)\n",
        "\n",
        "num_comp = pca.n_components_\n",
        "print('Number of components was used:', num_comp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P_uOBzD6hiW"
      },
      "source": [
        "Dùng X_train_pca để thử tái tạo lại các tấm hình gốc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrEUMTTP6qxL"
      },
      "source": [
        "# TODO\n",
        "# 1. Tái tạo lại hình gốc từ X_train_pca\n",
        "# 2. Đặt tên biến là X_train_projected\n",
        "X_train_projected = pca.inverse_transform(X_train_pca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WvrnX5K9CQu"
      },
      "source": [
        "Visualize ảnh gốc versus ảnh được tái tạo.\n",
        "\n",
        "Trước khi plot những tấm hình này ra, ta cần gọi hàm ``inverse_transform`` của ``StandardScaler`` để scale giá trị của các điểm ảnh về lại ban đầu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v456v5pCr8p"
      },
      "source": [
        "# TODO\n",
        "# 1. Use inverse_transform()\n",
        "X_train_visualize = scaler.inverse_transform(X_train)\n",
        "X_train_projected_visualize = scaler.inverse_transform(X_train_projected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ0YoEzG5Po-"
      },
      "source": [
        "fig, axs = plt.subplots(10,2)\n",
        "fig.set_figheight(50)\n",
        "fig.set_figwidth(10)\n",
        "for i in range(10):\n",
        "  target = np.random.choice(np.where(y_train == i)[0])\n",
        "  axs[i][0].grid('off')\n",
        "  axs[i][0].axis('off')\n",
        "  axs[i][1].grid('off')\n",
        "  axs[i][1].axis('off')\n",
        "  axs[i][0].set_title('Original')\n",
        "  axs[i][1].set_title(str(num_comp)+' components PCA')\n",
        "  axs[i][0].imshow(X_train_visualize[target].reshape(img_H, img_W), cmap='gray')\n",
        "  axs[i][1].imshow(X_train_projected_visualize[target].reshape(img_H, img_W), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCnMv_V89ch2"
      },
      "source": [
        "## 4.2.3 [1-point] Áp dụng Logistic Regression (sklearn) để phân loại"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-RofWFJAeQr"
      },
      "source": [
        "Ở bước này ta dùng sklearn để thực hiện phân loại các tấm hình. Do sử dụng sklearn nên sẽ có các điểm khác biệt so với khi dùng Keras:\n",
        "\n",
        "*   Không cần one hot encoding label (y)\n",
        "*   Mặc dù tên là Logistic Regression nhưng trong sklearn có thể được áp dụng cho bài toán phân loại nhiều class\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcWskDHvAXCg"
      },
      "source": [
        "Import Logistic Regression từ sklearn\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCpvZZO6XME"
      },
      "source": [
        "# TODO\n",
        "# 1. Import thư viện\n",
        "# 2. Khởi tạo LogisticRegression với parameter max_iter=800 \n",
        "# 3. Đặt tên biến là model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(max_iter=800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMwwPmSeDRD9"
      },
      "source": [
        "Cell bên dưới sẽ train model trên Train Set và in ra thời gian train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBrq3Nie_WVQ"
      },
      "source": [
        "import time \n",
        "\n",
        "start = time.time()\n",
        "model.fit(X_train_pca, y_train)\n",
        "end = time.time()\n",
        "print('Training Time:',str(end-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSH1rHQmEDOY"
      },
      "source": [
        "Đánh giá performance của model dùng accuracy_score() trên Train Set và Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B1auI2L90ml"
      },
      "source": [
        "# TODO:\n",
        "# 1. Tìm cách sử dụng hàm accuracy_score của SkLearn\n",
        "# 2. In ra Accuracy của model trên Train Set và Test Set\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_train_pred = model.predict(X_train_pca)\n",
        "y_test_pred = model.predict(X_test_pca)\n",
        "print('Model accuracy on Train Set:', accuracy_score(y_train,y_train_pred))\n",
        "print('Model accuracy on Test Set:', accuracy_score(y_test,y_test_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5BmOcue_1IQ"
      },
      "source": [
        "Thử thay đổi lượng thông tin giữ lại khi khai báo PCA ở trên và train lại để xem kết quả thay đổi như thế nào\n",
        "\n",
        "Điền kết quả vào bảng Report bên dưới"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urGO4IeNFgPc"
      },
      "source": [
        "## 4.2.4 [1-point] Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2dHZu9wEhE4"
      },
      "source": [
        "Double click vào cell này để điền kết quả\n",
        "\n",
        "Information Retained | Number of components | Train time | Accuracy on Test Set\n",
        "--- | --- | --- | --- \n",
        "100% (không PCA) | Không điền  | ?  | ? \n",
        "99% | ?  | ?  | ? \n",
        "95% | ?  | ?  | ? \n",
        "90% | ?  | ?  | ? \n",
        "80% | ?  | ?  | ? \n",
        "70% | ?  | ?  | ? \n",
        "50% | ?  | ?  | ? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO0noM3mIW8m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}