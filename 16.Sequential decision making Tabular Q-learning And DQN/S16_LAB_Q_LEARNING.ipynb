{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S16_LAB_Q-LEARNING.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PbTJ34armvB"
      },
      "source": [
        "<h1><center>\n",
        "\n",
        "![](https://i.imgur.com/jrRGMfx.png)\n",
        "\n",
        "AI PRACTITIONER COURSE \n",
        "\n",
        "PRACTICE LAB - SESSION 16 - Sequential decision making: Tabular Q-learning & DQN \n",
        "\n",
        "**&copy; 2020 VTCA-COTAI. Internal Use Only.**\n",
        "\n",
        "</center></h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSA--2K8L4IQ"
      },
      "source": [
        "# Giới thiệu Reinforcement learning (Rl) - learning to act\n",
        "\n",
        "Reinforcement learning - học tăng cường được định nghĩa một cách tổng quan nhất là các bài toán trong đó mục tiêu là để tối đa hóa phần thưởng trong dài hạn. Ví dụ khi chơi cờ, điều mà người chơi có kinh nghiệm phải tính toán là khi đi mỗi nước cờ hoặc cụm các nước cờ bằng cách nào đấy đều phải đóng góp đến kết quả toàn cục.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cf/CaroVN.jpg\" height=\"270\">\n",
        "</center>\n",
        "\n",
        "Mô hình của DQN gồm *tác nhân (agent)* tương tác với *môi trường (environment)* tại *trạng thái (state)* thông qua các *hành động (action)* tại thời điểm $t$ và nhận về *phần thưởng (reward)*. Sau mỗi bước hành động tác nhân sẽ chuyển tiếp sang trạng thái mới. Một vòng lặp các phản ứng qua lại giữa tác nhân - môi trường được mô phỏng như hình sau:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/904/1*WOYVzYnF-rbdcgZU2Wt9Yw.png\" height=\"200\">\n",
        "</center>\n",
        "\n",
        "Trong đó:\n",
        "\n",
        "- Trạng thái thể hiện tình trạng của tác nhân và môi trường tại thời điểm $t$.\n",
        "\n",
        "- Tập hợp bộ hành động tác nhân có thể làm tại mỗi trạng thái.\n",
        "\n",
        "- Tập hợp bộ phần thưởng cho từng trạng thái - hành động.\n",
        "\n",
        "***Tổng quan là:***\n",
        "\n",
        "> *Với trạng thái hiện tại, bộ hành động và tập hợp các phần thưởng tương ứng đã được quy định trước, máy sẽ học cách chọn ra hành động để tối đa hóa phần thưởng nhận được trong tương lai.*\n",
        "\n",
        "Vậy chắc chắn là ở đây đang tồn tại 1 cái hàm nào đó có đầu vào đầu ra. Đầu vào bao gồm các trạng thái, bộ action có thể thực hiện tại từng trạng thái, reward cho mỗi cặp trạng thái - hành động. Còn đầu ra là hành động nên làm.\n",
        "\n",
        "Và cái hàm mà nhận vào 1 bảng (ta gọi là bảng *Q-value*) và trả ra hành động tương ứng gọi là *hàm chiến lược (policy)*, ký hiệu là $\\pi$. \n",
        "\n",
        "***Như vậy:***\n",
        "\n",
        "> *Thuật toán sẽ đi tìm hàm chiến lược tối ưu kia. Tối ưu dựa trên tiêu chí là, nó sẽ học cách tính toán hành động nào nên làm để tối đa phần thưởng nhiều nhất có thể trong tương lai.*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbq6OVRuuIh9"
      },
      "source": [
        "# Code với game Hồ băng\n",
        "\n",
        "Mô tả: Game hồ băng, chia làm 16 ô vuông gồm ô bắt đầu, các ô vuông có băng hoặc không băng và ô đích. Ô bắt đầu chú thích là `S`, ô có băng chú thích là `F`, ô là một cái hố chú thích là `H`, ô đích đến chú thích là `G`. Nhiệm vụ của bạn là di chuyển từ `S` đến `G`. Nếu đi lọt vào ô hố thì game over!\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://miro.medium.com/max/884/1*MCjDzR-wfMMkS0rPqXSmKw.png)\n",
        "<center>\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/842/1*Qp14HWQfOeE2UoSxrxCxAg.png\" height=\"270\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "### Q-values table\n",
        "\n",
        "|State(cell)|Left|Down|Right|Up|\n",
        "|---|---|---|---|---|\n",
        "|00 | 0 | 0 | 0 | 0 |\n",
        "|01 | 0 | 0 | 0 | 0 |\n",
        "|...| 0 | 0 | 0 | 0 |\n",
        "|15 | 0 | 0 | 0 | 0 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1KI19m2oWz0"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import gym\n",
        "import random\n",
        "from gym.envs.registration import register\n",
        "\n",
        "## Load and setup the environment\n",
        "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
        "#To start, we need to reset to the initial (default) state\n",
        "env.reset()\n",
        "#Get the total number of possibles states (obervations)\n",
        "states = env.observation_space.n\n",
        "#Get the total number of actions available to the agent\n",
        "actions = env.action_space.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR7b4S9synTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a02c52f5-fe9e-4615-c2d1-bf376a78f91a"
      },
      "source": [
        "#To visualize the current state\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr3rqsfCykum"
      },
      "source": [
        "## Initialize Q-table structure\n",
        "#---->TODO<----: create a table to contain expected values at given states\n",
        "Q = np.zeros(shape = (16,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTqLqnqA9aYs"
      },
      "source": [
        "## For Q-learning\n",
        "epsilon = 0.99 #For the epsilon-greedy approach\n",
        "gamma = 0.9 #Discount factor\n",
        "eta = 0.8 #Learning rate\n",
        "#The maximum amount of times we'll run the game\n",
        "total_episodes = 1000\n",
        "\n",
        "#The maximum steps we'll run for every episode \n",
        "max_steps = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtQT5ZfFbl_j"
      },
      "source": [
        "# Phân tích theo TEFPA\n",
        "\n",
        "- Task: input là state, output là optimal action.\n",
        "\n",
        "- E: \n",
        "  + Input: trạng thái hiện tại, hành động kế tiếp\n",
        "  + Output: phần thưởng ở hiện tại.\n",
        "\n",
        "- F: hàm chiến lược $\\pi$. \n",
        "\n",
        "- P: Phương trình Bellman:\n",
        "\n",
        "$$\n",
        "Q^{target}(s_t,a_t) \\approx r_t + \\gamma max_{a_{t+1}}(Q(s_{t+1},a_{t+1}))\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "P = \\Delta{Q} = \\hat{Q}(s_t,a_t) - Q^{target}(s_t,a_t)\n",
        "$$\n",
        "\n",
        "- A:\n",
        "\n",
        "$$\n",
        "\\hat{Q}(s_t,a_t) = \\hat{Q}(s_t,a_t) - \\eta*\\Delta{Q} \\\\\n",
        "= \\hat{Q}(s_t,a_t) +\\eta[r_t + \\gamma max_{a_{t+1}}(Q(s_{t+1},a_{t+1})) - \\hat{Q}(s_t,a_t)]\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRlxKxIhyf1a"
      },
      "source": [
        "#---->TODO:\n",
        "def choose_action(Q,state, epsilon):\n",
        "    #Randomly generate a number between 0 and 1 and see if it's smaller than epsilon\n",
        "    ep = np.random.uniform(0,1)\n",
        "    #If smaller, a random action is chosen using env.action_space.sample()\n",
        "    if ep < epsilon :\n",
        "        return env.action_space.sample()\n",
        "    #If greater, we choose the action having the maximum value in the Q-table for that state: a.k.a best action\n",
        "    else :\n",
        "        return np.argmax(Q[state])\n",
        "\n",
        "#---->TODO: code Q-learning based on Q formula\n",
        "#input: old_value: Q[s,a] \n",
        "#       max_value_of_next_state: max(a') Q[s',a'] \n",
        "#output: new_value: Q[s,a] updated\n",
        "def update_Q(old_value, max_value_of_next_state, reward):\n",
        "    return old_value + eta*(reward+gamma*max_value_of_next_state-old_value)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J55TpUgw45bz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ddade4b-c6fb-4111-b41f-19acb384be53"
      },
      "source": [
        "## Start\n",
        "rList = []\n",
        "for episode in range(total_episodes):\n",
        "    #Store the initial state using env.reset()\n",
        "    state = env.reset()\n",
        "    #Store the number of steps\n",
        "    t = 0\n",
        "    #Reduce epsilon gradually\n",
        "    epsilon = max(0.01, epsilon-0.001)\n",
        "\n",
        "    rAll = 0\n",
        "    while t < max_steps:\n",
        "        # env.render()\n",
        "        #---->TODO: Choose appropriate action\n",
        "        action = choose_action(Q,state,epsilon)\n",
        "        #After the action taken in the environment, the reward for that action and next_state is returned\n",
        "        #env.step() returns a tuple\n",
        "        #done (bool type) returns True if the espisode is finished\n",
        "        #info (dict type) stores the extra information for debugging purposes\n",
        "        next_state, reward, done, info = env.step(action) \n",
        "        #---->TODO: Append reward into rList\n",
        "        rList.append(reward)\n",
        "        #Update Q-table \n",
        "        old_value = Q[state,action]\n",
        "        max_value_of_next_state = max(Q[next_state])\n",
        "        #---->TODO: Update Q-value\n",
        "        Q[state,action] = update_Q(old_value,max_value_of_next_state,reward)\n",
        "\n",
        "        #Set the current state as the next state   \n",
        "        state = next_state\n",
        "        t += 1\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    rList.append(rAll)\n",
        "print(Q)\n",
        "    # print(Q, episode, epsilon)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.531441 0.59049  0.59049  0.531441]\n",
            " [0.531441 0.       0.6561   0.59049 ]\n",
            " [0.59049  0.729    0.59049  0.6561  ]\n",
            " [0.6561   0.       0.59049  0.59049 ]\n",
            " [0.59049  0.6561   0.       0.531441]\n",
            " [0.       0.       0.       0.      ]\n",
            " [0.       0.81     0.       0.6561  ]\n",
            " [0.       0.       0.       0.      ]\n",
            " [0.6561   0.       0.729    0.59049 ]\n",
            " [0.6561   0.81     0.81     0.      ]\n",
            " [0.729    0.9      0.       0.729   ]\n",
            " [0.       0.       0.       0.      ]\n",
            " [0.       0.       0.       0.      ]\n",
            " [0.       0.81     0.9      0.729   ]\n",
            " [0.81     0.9      1.       0.81    ]\n",
            " [0.       0.       0.       0.      ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqIB-jPY9u-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2107a95-2dd6-453c-f3c3-09ff85046895"
      },
      "source": [
        "## Visualize the movements\n",
        "\n",
        "#---->TODO: get a list of best actions manually\n",
        "\n",
        "#---->TODO: visualize\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "state = env.reset()\n",
        "env.render()\n",
        "clear_output(wait=True)\n",
        "time.sleep(1)\n",
        "while True:\n",
        "  action = np.argmax(Q[state])\n",
        "  next_state, reward, done, _ = env.step(action)\n",
        "  env.render()\n",
        "  clear_output(wait=True)\n",
        "  time.sleep(1)\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "  state = next_state\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGp1x4eZ1PFl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}