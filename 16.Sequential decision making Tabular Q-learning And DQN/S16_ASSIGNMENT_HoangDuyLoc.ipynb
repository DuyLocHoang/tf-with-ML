{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S16_ASSIGNMENT_HoangDuyLoc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5GsgqejoHWP"
      },
      "source": [
        "<h1><center>\n",
        "\n",
        "![](https://i.imgur.com/jrRGMfx.png)\n",
        "\n",
        "AI PRACTITIONER COURSE \n",
        "\n",
        "ASSIGNMENT - SESSION 16 - Sequential decision making: Tabular Q-learning & DQN\n",
        "\n",
        "**&copy; 2020 VTCA-COTAI. Internal Use Only.**\n",
        "\n",
        "</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQsO1y1fzaPV"
      },
      "source": [
        "# 15.1 [2 Points] Theory Review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcHscd8CoS1Q"
      },
      "source": [
        "## 15.1.1 [1 Point] Reinforcement Learning\n",
        "\n",
        "- What are the main differences between RL and Planning algorithms? Given action value function $Q^*(s,a)$, what is the optimal policy $\\pi^*(a|s) ~ \\forall s,a$?\n",
        "- Các điểm khác biệt chính giữa các giải thuật AI học-tăng-cường (học điều khiển, học ra quyết định) và lên kế hoạch? Từ hàm giá trị $Q^*(s,a)$ tìm hàm ra quyết định tối ưu $\\pi^*(a|s) ~ \\forall s,a$ ra sao?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuK9EKUzsmZe"
      },
      "source": [
        "Điểm khác biệt: \n",
        "Planning : T,R được cho trước\n",
        ". RL : Không biết T và R\n",
        "\n",
        "Tối ưu :$\\pi^*(a|s)$ = argmax$Q^*(s,a)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I8O5KxYzeAk"
      },
      "source": [
        "## 15.1.2 [1 Point] Q-Learning\n",
        "\n",
        "- Write down update equation of Q-learning and compare it with planning by Dynamic Programming: what are the main differences? What is the policy $\\pi(a|s)$ the agent follows while learning from interactions in Q-learning?\n",
        "- Viết phương trình cập nhật (học) tham số trong Q-learning, so với phương trình lên kế hoạch bằng quy-hoạch-động (DP) có gì khác? Hàm ra quyết định $\\pi(a|s)$ mà AI thực hiện khi học tương tác trong Q-learning là gì?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3JKuRw1tsgl"
      },
      "source": [
        "Phương trình cập nhật : \n",
        "\n",
        "$$\n",
        "\\hat{Q}(s_t,a_t) = \\hat{Q}(s_t,a_t) - \\eta*\\Delta{Q} \\\\\n",
        "= \\hat{Q}(s_t,a_t) +\\eta[r_t + \\gamma max_{a_{t+1}}(Q(s_{t+1},a_{t+1})) - \\hat{Q}(s_t,a_t)]\n",
        "$$\n",
        "\n",
        "MDP xác định trạng thái của môi trường, hành động mà tác nhân có thể thực hiện, phần thưởng và kỳ vọng của nó với hành động và trạng thái tiếp theo. Q- learning là một thuật toán để học chính sách dẫn đến lợi nhuận tối ưu khi tác nhân hoạt động trong một môi trường\n",
        "\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA5YCgTVmgQM"
      },
      "source": [
        "# 15.2 [4 Points] Coding Practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvjqPdEUmoT6"
      },
      "source": [
        "## 15.2.1 [1 Point] Khởi tạo và tìm hiểu Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiyoxaBznnXC"
      },
      "source": [
        "**Giới thiệu environment**\n",
        "\n",
        "https://gym.openai.com/envs/Taxi-v3/\n",
        "\n",
        "https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuPgXgMuL3kL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5990a41f-06e6-4ebb-918c-7ae50b86d289"
      },
      "source": [
        "# Tham khảo: \n",
        "import gym \n",
        "\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBzCswwCQZXZ"
      },
      "source": [
        "### TODO: Các chữ trên bản đồ có ý nghĩa gì?\n",
        "\n",
        "### TODO: Màu tím hồng và chữ màu xanh dương nghĩa là gì?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sACfwIZGG8VY"
      },
      "source": [
        "Các chữ trên bản đồ là destinations.\n",
        "Màu tím hồng : Vị trí hiện tại của taxi\n",
        "Blue : Vị trí hiện tại của passenger\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRGzTGZFMOQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70180559-803b-4500-83df-7591d51abed6"
      },
      "source": [
        "env.reset() # reset environment to a new, random state\n",
        "env.render()\n",
        "\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "|\u001b[43m \u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31EwDUOiQXST"
      },
      "source": [
        "### TODO: Có tất cả 6 hành động khả dĩ (action). Đó là những hành động nào?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT0qAISaHYAE"
      },
      "source": [
        "    There are 6 discrete deterministic actions:\n",
        "    - 0: move south\n",
        "    - 1: move north\n",
        "    - 2: move east\n",
        "    - 3: move west\n",
        "    - 4: pickup passenger\n",
        "    - 5: drop off passenger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDh4yBRgmuwP"
      },
      "source": [
        "## 15.2.2 [3 Points] Training and Evaluating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgSJaA6Kod3X"
      },
      "source": [
        "**Đọc source code bên dưới và hoàn thành tất cả phần #TODO**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JT3l5ypN_Fb"
      },
      "source": [
        "import numpy as np\n",
        "# TODO: Khởi tạo q_table\n",
        "q_table = np.zeros(shape = (500,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCX-D9Z3OFnh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c02ea129-d858-4ac9-d81b-908b2684566e"
      },
      "source": [
        "%%time\n",
        "\"\"\"Training the agent\"\"\"\n",
        "\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "\n",
        "# For plotting metrics\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "\n",
        "for i in range(1, 100001):\n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            # TODO: Explore action space\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # TODO: Exploit learned values\n",
        "            action = np.argmax(q_table[state])\n",
        "\n",
        "        # TODO: interact with env\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        old_value = q_table[state,action]\n",
        "\n",
        "        # TODO: calculate target\n",
        "        next_max = max(q_table[next_state])\n",
        "        \n",
        "        # TODO: calculate new value for current q table cell\n",
        "        new_value = old_value + alpha*(reward + gamma*next_max - old_value)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "CPU times: user 43.3 s, sys: 9.89 s, total: 53.1 s\n",
            "Wall time: 44.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esmno1hgOjJV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36cce87b-70da-4ad6-a865-8ff72a809dff"
      },
      "source": [
        "# Evaluate agent's performance after Q-learning\n",
        "# TODO:\n",
        "# - Run for 100 episodes\n",
        "# - Calculate average epochs (timesteps) \bper episode\n",
        "# - Calculate average penalties per episode\n",
        "\n",
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        # TODO: choose best action\n",
        "        action = np.argmax(q_table[state])\n",
        "        # TODO: interact with env\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 12.95\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhzajFVhSefh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70ee4b33-3652-4084-e434-32ddcf10aa2c"
      },
      "source": [
        "# TODO:\n",
        "# Implement algorithm again where:\n",
        "# - Initial value of epsilon is 0.9\n",
        "# - Epsilon value is decayed every 100 episodes\n",
        "# - Min value of epsilon is 0.1\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "\n",
        "epsilon =0.9\n",
        "# For plotting metrics\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "\n",
        "for i in range(1, 100001):\n",
        "    state = env.reset()\n",
        "    if i/101 in [i for i in range(100)] :\n",
        "        epsilon = max(0.1,epsilon-0.001)\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            # TODO: Explore action space\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # TODO: Exploit learned values\n",
        "            action = np.argmax(q_table[state])\n",
        "\n",
        "        # TODO: interact with env\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        old_value = q_table[state,action]\n",
        "\n",
        "        # TODO: calculate target\n",
        "        next_max = max(q_table[next_state])\n",
        "        \n",
        "        # TODO: calculate new value for current q table cell\n",
        "        new_value = old_value + alpha*(reward + gamma*next_max - old_value)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN8mX_JhWZAw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "outputId": "898b57b8-b1fb-422d-a1e6-7ae2df818f8a"
      },
      "source": [
        "# TODO:\n",
        "# Visualize the evaluation process\n",
        "# - Print the info of episode, timestamp, state, action, reward per frame\n",
        "# - Should delay around 1 second between 2 frames (easy to watch)\n",
        "import time\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Episode: {frame['episode']}\")\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        time.sleep(1)\n",
        "\n",
        "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
        "\n",
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "frames = []\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        # TODO: choose best action\n",
        "        action = np.argmax(q_table[state])\n",
        "        # TODO: interact with env\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "         # TODO: Put each rendered frame into dict for animation\n",
        "         #       Using env.reander(mode='ansi') to generate each frame\n",
        "        frames.append({\n",
        "            'frame': env.render(mode = 'ansi'),\n",
        "            'episode': episodes, \n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'reward': reward\n",
        "            }\n",
        "        )\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
        "\n",
        "print_frames(frames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "Episode: 100\n",
            "Timestep: 30\n",
            "State: 271\n",
            "Action: 3\n",
            "Reward: -1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-0d8ad46b8173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Average penalties per episode: {total_penalties / episodes}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mprint_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-0d8ad46b8173>\u001b[0m in \u001b[0;36mprint_frames\u001b[0;34m(frames)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Action: {frame['action']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Reward: {frame['reward']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\"\"\"Evaluate agent's performance after Q-learning\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azhRACjROuIy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}